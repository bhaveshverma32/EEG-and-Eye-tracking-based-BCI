{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3b6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"F:/eegonlinedata/22_INSIGHT2_170315_2022.11.09T12.53.37+05.30.edf\"\n",
    "event_path=\"F:/eegonlinedata/rahul_sai_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed9a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_realtime import LSLClient, MockLSLStream\n",
    "import pylsl as lsl\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.signal import wiener\n",
    "from scipy.signal import welch\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# !nvidia-smi\n",
    "import torch\n",
    "import mne\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd0cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSIGHT2-A3D20043\n",
      "Client: Waiting for server to start\n",
      "Looking for LSL stream INSIGHT2-A3D20043...\n",
      "Found stream 'EmotivDataStream-EEG' via INSIGHT2-A3D20043...\n",
      "Client: Connected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne_realtime.lsl_client.LSLClient at 0x214d40dad90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find host id\n",
    "streams = lsl.resolve_streams(wait_time=min(0.1, 3))\n",
    "\n",
    "for stream_info in streams:\n",
    "            print(stream_info.source_id())\n",
    "###########\n",
    "\n",
    "host= 'INSIGHT2-A3D20043'\n",
    "wait_max=5\n",
    "client = LSLClient( info=None,host=host, wait_max=wait_max)\n",
    "client.start()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a489e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from F:\\eegonlinedata\\22_INSIGHT2_170315_2022.11.09T12.53.37+05.30.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 209663  =      0.000 ...  1637.992 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 45 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 45.00 Hz\n",
      "- Upper transition bandwidth: 11.25 Hz (-6 dB cutoff frequency: 50.62 Hz)\n",
      "- Filter length: 423 samples (3.305 sec)\n",
      "\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#RECORDED EEG PROCESSING\n",
    "\n",
    "# df = pd.read_csv(event_path)\n",
    "\n",
    "npe= np.genfromtxt(event_path, delimiter=',')\n",
    "npevents=np.zeros((npe.shape[0],3))\n",
    "npevents[:,0]=npe[:,0]*128\n",
    "npevents[:,2]=npe[:,1]\n",
    "npevents=npevents.astype(int)\n",
    "\n",
    "datax=mne.io.read_raw_edf(file_path,preload=True)\n",
    "datax.annotations\n",
    "\n",
    "#channel location\n",
    "channelnames=datax.ch_names\n",
    "listo=list(set(channelnames)-set(['AF3', 'T7', 'Pz', 'T8', 'AF4']))\n",
    "# datax.info['bads'] += listo\n",
    "datax.drop_channels(listo)\n",
    "#filter\n",
    "datax.filter(l_freq=1,h_freq=45)\n",
    "#time remove\n",
    "datax.crop(tmin=5)\n",
    "#rereferencing data\\\n",
    "datax.set_eeg_reference()\n",
    "event_dict={\n",
    " 'rh':12,\n",
    " 'lh':13,\n",
    " 'rl':14,\n",
    " 'll':11,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a918cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "280 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 280 events and 321 original time points ...\n",
      "0 bad epochs dropped\n",
      "data (280, 5, 321)\n",
      "(224, 128) (56, 2)\n",
      "{0, 1, 2, 3}\n",
      "torch.Size([224, 128]) torch.float32 torch.Size([224, 2]) torch.float32 (280,)\n"
     ]
    }
   ],
   "source": [
    "from mne.decoding import CSP # Common Spatial Pattern Filtering\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "\n",
    "tmin, tmax = -.5, 2\n",
    "# left_hand = 769,right_hand = 770,foot = 771,tongue = 772\n",
    "event_id = dict({ 'rh':12, 'lh':13, 'rl':14, 'll':11,})\n",
    "epochs = mne.Epochs(datax, npevents, event_id, tmin, tmax, proj=True, baseline=None, preload=True)\n",
    "# epochs=mne.make_fixed_length_epochs(datax,duration=25,overlap=0)\n",
    "# epochs=epochs.get_data()\n",
    "\n",
    "\n",
    "labels = epochs.events[:,-1] - 11  \n",
    "enc = OneHotEncoder()\n",
    "Yall = enc.fit_transform(labels.reshape(-1,1)).toarray()\n",
    "\n",
    "Y11= (epochs.events[:,-1]==11)*1\n",
    "Y11= enc.fit_transform(Y11.reshape(-1,1)).toarray()\n",
    "\n",
    "\n",
    "data = epochs.get_data()\n",
    "print(\"data\",data.shape)\n",
    "# XallF = np.concatenate((std(data), ptp(data), var(data), minim(data), maxim(data), argminim(data), argmaxim(data), mean_square(data), rms(data),abs_diffs_signal(data), skewness(data),  kurtosis(data),concatenate_features(data)\n",
    "# ), axis=1)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler() \n",
    "# Xscaled = scaler.fit_transform(XallF)\n",
    "chnls1=1\n",
    "data1=data[:,chnls1,0:128]\n",
    "    \n",
    "# data2=data1.reshape((:,128))\n",
    "\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = model_selection.train_test_split(data1, Y11, train_size=0.80, test_size=0.20, random_state=101)\n",
    "# X_train=X_train.reshape(224,641*5)\n",
    "# X_test=X_test.reshape(56,641*5)\n",
    "\n",
    "X_train, X_test, y_train, y_test  =  X_train2, X_test2, y_train2, y_test2\n",
    "# X_val, y_val = X_test,y_test\n",
    "\n",
    "# print(Xall.shape,Yall.shape, Y11.shape,type(Y11), type(Xall), type(Yall))\n",
    "print( X_train2.shape, y_test2.shape)\n",
    "print(set(labels))\n",
    "\n",
    "trX = torch.from_numpy(X_train).float()\n",
    "teX = torch.from_numpy(X_test).float()\n",
    "trY = torch.from_numpy(y_train).long()\n",
    "teY = torch.from_numpy(y_test).long()\n",
    "trY = torch.from_numpy(y_train).float()\n",
    "teY = torch.from_numpy(y_test).float()\n",
    "\n",
    "print(trX.shape,trX.dtype,trY.shape,trY.dtype,labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be986996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sigm(self.i2h(x))\n",
    "        x = self.sigm(self.h2h(x))\n",
    "        x = self.sigm(self.h2o(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = Net(128, 20, 2)\n",
    "epochsN = 10\n",
    "batch_size = 1\n",
    "learning_rate = 0.1\n",
    "momentum = .01\n",
    "\n",
    "\n",
    "#define loss and optimizer\n",
    "criterion=nn.BCELoss()\n",
    "# optimizer=optim.RMSprop(net.parameters(),lr=1e-4),\n",
    "\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "\n",
    "\n",
    "#training function\n",
    "\n",
    "def train(model, criterion, optimizer, x, y):\n",
    "    x = Variable(x, requires_grad=False)\n",
    "    y = Variable(y, requires_grad=False)\n",
    "    # print(x.shape,y.shape,y)\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    fx = model.forward(x)\n",
    "    loss = criterion(fx, y)\n",
    "    # print(loss,fx,y)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data\n",
    "\n",
    "\n",
    "#make predictions\n",
    "\n",
    "def predict(model, x, y):\n",
    "    x = Variable(x, requires_grad=False)\n",
    "    outputs = model(x)\n",
    "    _, predicted = torch.max(outputs.data, 1) #for each output, get the predicted value (torch.max returns (index, value) tuple)\n",
    "    _,in_real= torch.max(y,1)\n",
    "    correct = (predicted == in_real) #how many predicted values equal the labels\n",
    "    return correct.sum() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8364f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the training\n",
    "\n",
    "num_examples = trX.shape[0]\n",
    "# num_batches = num_examples // batch_size\n",
    "num_batches=1\n",
    "\n",
    "for e in range(1, epochsN+1):\n",
    "    loss = 0.\n",
    "    for k in range(num_batches):\n",
    "        start, end = k * batch_size, (k + 1) * batch_size\n",
    "        loss += train(net, criterion, optimizer, trX[start:end], trY[start:end])\n",
    "    correct = predict(net, teX, teY)\n",
    "#     plot_loss.append(loss/num_batches)\n",
    "#     plot_correct.append(correct/teX.shape[0]*100)\n",
    "#     print(\"Epoch %02d, loss = %f, accuracy = %.2f%%\" % (e, loss / num_batches, correct/teX.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4797c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "tensor([[0.6925, 0.2649]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sfreq=128\n",
    "epoch = client.get_data_as_epoch(n_samples=sfreq,picks=[3,4,5,6,7]);\n",
    "data = epoch.get_data();\n",
    "#     print(data.shape, np.mean(data))\n",
    "data = wiener(data, (1,1, 40));\n",
    "chnls1 = [1]\n",
    "data1 = data[:,chnls1,:]\n",
    "data2 = data1.reshape((1,128))\n",
    "data3 = torch.from_numpy(data2).float()\n",
    "output = net(data3)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5c5bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "x_dim: 1920, y_dim: 1080\n"
     ]
    }
   ],
   "source": [
    "# Initiate EyeTracking Pipeline\n",
    "\n",
    "\"\"\"\n",
    "Stream Pupil gaze coordinate data using zmq to control a mouse with your eye.\n",
    "Please note that marker tracking must be enabled, and in this example we have named the surface \"screen.\"\n",
    "You can name the surface what you like in Pupil capture and then write the name of the surface you'd like to use on line 17.\n",
    "\"\"\"\n",
    "print(\"start\")\n",
    "# specify the name of the surface you want to use\n",
    "surface_name = \"Surface1\"\n",
    "\n",
    "## install dependencies\n",
    "# pip3 install zmq msgpack pyuserinput\n",
    "\n",
    "import zmq\n",
    "from msgpack import loads\n",
    "import subprocess as sp\n",
    "from platform import system\n",
    "\n",
    "try:\n",
    "    from pymouse import PyMouse\n",
    "except ImportError:\n",
    "    msg = \"\"\"\n",
    "    Please install PyMouse from PyUserInput\n",
    "    https://github.com/PyUserInput/PyUserInput\n",
    "\n",
    "    pip install PyUserInput\n",
    "    \"\"\"\n",
    "    print(msg)\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "m = PyMouse()\n",
    "\n",
    "\n",
    "context = zmq.Context()\n",
    "# open a req port to talk to pupil\n",
    "addr = \"127.0.0.1\"  # remote ip or localhost\n",
    "req_port = \"50020\"  # same as in the pupil remote gui\n",
    "req = context.socket(zmq.REQ)\n",
    "req.connect(\"tcp://{}:{}\".format(addr, req_port))\n",
    "# ask for the sub port\n",
    "req.send_string(\"SUB_PORT\")\n",
    "sub_port = req.recv_string()\n",
    "\n",
    "# open a sub port to listen to pupil\n",
    "sub = context.socket(zmq.SUB)\n",
    "sub.connect(\"tcp://{}:{}\".format(addr, sub_port))\n",
    "sub.setsockopt_string(zmq.SUBSCRIBE, f\"surfaces.{surface_name}\")\n",
    "\n",
    "smooth_x, smooth_y = 0.5, 0.5\n",
    "\n",
    "# screen size\n",
    "x_dim, y_dim = m.screen_size()\n",
    "print(\"x_dim: {}, y_dim: {}\".format(x_dim, y_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fd839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "# Start The BCI\n",
    "\n",
    "prev_time=time.time()\n",
    "sfreq=128\n",
    "info={'sfreq':128}\n",
    "n_epochs=4000\n",
    "\n",
    "for ii in range(n_epochs):\n",
    "#     time.sleep(0.01)\n",
    "\n",
    "    \n",
    "    if time.time()-prev_time>2:\n",
    "        prev_time=time.time()\n",
    "            \n",
    "        epoch = client.get_data_as_epoch(n_samples=sfreq,picks=[3,4,5,6,7]);\n",
    "        data = epoch.get_data();\n",
    "    #     print(data.shape, np.mean(data))\n",
    "        chnls1 = [1]\n",
    "        data = data[:,chnls1,:]\n",
    "        data1 = wiener(data, (1,1, 40));\n",
    "        data2 = data1.reshape((1,128))\n",
    "        data3 = torch.from_numpy(data2).float()\n",
    "        output = net(data3)\n",
    "    \n",
    "#         print(output)\n",
    "#         if output[0,0]>output[0,1]:\n",
    "#             pyautogui.click(button='right', clicks=1, interval=0.05)\n",
    "#         else:\n",
    "#             pyautogui.click(button='left', clicks=1, interval=0.05) \n",
    "\n",
    "    \n",
    "###########################################    \n",
    "\n",
    "    sub\n",
    "    topic, msg = sub.recv_multipart()\n",
    "\n",
    "    gaze_position = loads(msg, raw=False)\n",
    "    \n",
    "    if gaze_position[\"name\"] == surface_name:\n",
    "        gaze_on_screen = gaze_position[\"gaze_on_surfaces\"]\n",
    "#         print(gaze_on_screen)\n",
    "\n",
    "        if len(gaze_on_screen) > 0:\n",
    "\n",
    "            # there may be multiple gaze positions per frame, so you could average them\n",
    "            # raw_x = sum([i['norm_pos'][0] for i in gaze_on_screen])/len(gaze_on_screen)\n",
    "            # raw_y = sum([i['norm_pos'][1] for i in gaze_on_screen])/len(gaze_on_screen)\n",
    "\n",
    "            # or just use the most recent gaze position on the surface\n",
    "            raw_x, raw_y = gaze_on_screen[-1][\"norm_pos\"]\n",
    "\n",
    "            # smoothing out the gaze so the mouse has smoother movement\n",
    "            smooth_x += 0.35 * (raw_x - smooth_x)\n",
    "            smooth_y += 0.35 * (raw_y - smooth_y)\n",
    "            x = smooth_x\n",
    "            y = smooth_y\n",
    "\n",
    "            y = 1 - y  # inverting y so it shows up correctly on screen\n",
    "            x *= int(x_dim)\n",
    "            y *= int(y_dim)\n",
    "            # PyMouse or MacOS bugfix - can not go to extreme corners because of hot corners?\n",
    "            x = min(x_dim - 10, max(10, x))\n",
    "            y = min(y_dim - 10, max(10, y))\n",
    "\n",
    "            # print \"%s,%s\\n\" %(x,y)\n",
    "#             pyautogui.moveTo(int(x), int(y), duration = .01)\n",
    "            m.move(int(x), int(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd13cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4c72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fe524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
